**数据处理和分析的详细说明文档**

**1. 项目概述**

本项目旨在通过PySpark对一个约30GB的用户购买行为数据集进行多维度的数据挖掘分析。数据集包含用户ID、购买历史（JSON格式，内含商品ID、支付方式、支付状态、购买日期等）、商品目录等信息。分析任务包括商品类别关联规则挖掘、支付方式与商品类别关联及分布分析、时间序列模式挖掘以及退款模式分析。最终目标是提取有价值的业务洞察，并以图表和表格形式呈现结果。

**2. 环境与工具**

*   **编程语言**: Python 3.10
*   **核心框架**: Apache Spark 3.5.5 (通过PySpark API)
*   **主要Python库**:
    *   `pyspark.sql`: 用于DataFrame操作和SQL功能。
    *   `pyspark.ml.fpm`: 用于FP-Growth关联规则挖掘。
    *   `pandas`: 用于数据转换以便可视化。
    *   `matplotlib` & `seaborn`: 用于数据可视化。
    *   `networkx`: (在Task 1中) 用于绘制网络图。
*   **数据格式**: 原始用户数据为Parquet，商品目录为JSON。
*   **运行环境配置示例**:
    *   `spark.driver.memory`: "256g"
    *   `spark.executor.memory`: "256g"
    *   `spark.sql.shuffle.partitions`: "10000"
    (这些配置需要根据实际运行环境进行调整)

**3. 数据预处理流程**

预处理的目标是生成一个干净、结构化的DataFrame (`df_final_preprocessed`)，作为后续所有分析任务的基础。

*   **3.1 初始化SparkSession**:
    *   创建一个名为 "ComprehensiveDataMiningPipeline_Full" 的SparkSession。
    *   配置驱动器和执行器的内存，以及shuffle分区数和时间解析策略。

*   **3.2 商品目录处理 (`product_catalog.json`)**:
    *   读取多行JSON格式的商品目录文件。
    *   使用 `explode` 函数展开嵌套的 "products" 数组，提取每个商品的 `id` (重命名为 `product_id`)、`category` (商品小类，重命名为 `product_category`) 和 `price` (商品单价，重命名为 `product_unit_price`)。
    *   生成 `df_products` DataFrame。

*   **3.3 用户购买历史数据处理 (`30G_data_new/*.parquet`)**:
    *   读取Parquet文件集合，仅选择必要的 `id` 和 `purchase_history` 列以优化初始加载。
    *   定义 `purchase_history` JSON字符串的Schema，包括 `avg_price`, `items` (商品ID列表), `payment_method`, `payment_status`, `purchase_date`。
    *   使用 `from_json` 函数解析 `purchase_history` 列，生成包含结构化购买详情的 `ph_details` 列。

*   **3.4 展开商品项**:
    *   从 `ph_details` 中提取用户ID (`user_id`)、购买日期字符串 (`purchase_date_str`)、支付方式 (`payment_method`)、支付状态 (`payment_status`) 和原始交易平均价格 (`transaction_avg_price`)。
    *   使用 `explode` 函数展开 `ph_details.items` 数组，使得每个购买的商品ID (`purchased_item_id`) 成为单独的一行。
    *   生成 `df_exploded_purchases` DataFrame。

*   **3.5 与商品目录连接**:
    *   将 `df_exploded_purchases` 与 `df_products` DataFrame 进行左连接（`left join`），连接条件是 `purchased_item_id == product_id`。
    *   这样，每个购买的商品项都关联上了其对应的商品小类 (`product_category`) 和商品单价 (`product_unit_price`)。
    *   生成 `df_joined` DataFrame。

*   **3.6 商品大类映射**:
    *   定义一个Python字典 `category_to_major_map_dict`，将商品小类（如“智能手机”）映射到预定义的大类（如“电子产品”）。
    *   使用 `create_map` 和 `lit` 函数将此字典转换为Spark MapType表达式 `mapping_expr`。
    *   在 `df_joined` 的基础上，使用 `mapping_expr[col("product_category")]` 创建新的 `item_major_category` 列。
    *   对于未在映射字典中找到的小类，其大类被设置为 "其他"。
    *   生成 `df_with_major_category` DataFrame。

*   **3.7 最终预处理DataFrame生成 (`df_final_preprocessed`)**:
    *   将 `purchase_date_str` 列使用 `to_date` 函数转换为DateType，生成 `purchase_date` 列。
    *   选择最终分析所需的列: `user_id`, `purchase_date`, `payment_method`, `payment_status`, `purchased_item_id`, `item_minor_category` (来自 `product_category`), `item_major_category`, `item_unit_price` (来自 `product_unit_price`), `transaction_avg_price`。
    *   按 `user_id`, `purchase_date`, `purchased_item_id` 排序（可选，但有助于后续某些窗口操作的确定性）。
    *   **缓存 `df_final_preprocessed`** (`.cache()`)，因为它将在多个后续任务中被频繁使用。执行一次 `count()` 操作来触发缓存。
    *   打印其Schema和总行数进行验证。

*   **3.8 清理**: 对预处理过程中产生的不再需要的中间DataFrame（如 `df_main_raw`, `df_products` 等）执行 `unpersist()` 以释放内存。

**4. 分析任务详细说明**

*   **4.1 Task 1: 商品类别关联规则挖掘 (大类)**
    *   **目标**: 分析用户在同一订单中购买的不同商品**大类**之间的关联关系。
    *   **数据准备**:
        *   从 `df_final_preprocessed` 出发，按原始交易的唯一标识（`user_id`, `purchase_date`, `payment_method`, `payment_status`, `transaction_avg_price`）进行分组。
        *   使用 `collect_set("item_major_category")` 为每个交易创建一个包含所购商品大类的购物篮 (`items` 列)。
        *   过滤掉只包含少于2个不同大类的购物篮。
        *   缓存结果DataFrame (`baskets_df_filtered_task1`)。
    *   **FP-Growth算法**:
        *   设置 `minSupport = 0.02` 和 `minConfidence = 0.5`。
        *   在购物篮数据上训练FP-Growth模型。
    *   **结果提取与保存**:
        *   提取频繁项集，计算支持度百分比，显示并保存为CSV (`task1_frequent_itemsets.csv`)。数组列在保存CSV前使用 `concat_ws` 转换为字符串。
        *   提取关联规则，计算支持度、置信度、提升度（重命名为 `lift_val`），显示并保存为CSV (`task1_association_rules.csv`)。数组列（`antecedent`, `consequent`）在保存CSV前转换为字符串。
    *   **特定分析**: 筛选出前件或后件中包含“电子产品”的规则，并单独保存和可视化。
    *   **可视化**:
        *   为“电子产品”相关规则绘制独立的置信度条形图和提升度条形图。
        *   绘制综合条形图，条形长度代表置信度，颜色代表提升度。
        *   绘制网络图，节点为品类，边代表规则，边的宽度与提升度相关，颜色与置信度相关。
        *   所有图表均保存为PNG文件。
    *   **清理**: `unpersist()` 购物篮DataFrame。

*   **4.2 Task 2: 支付方式与商品类别分析**
    *   **Task 2.1.a: 支付方式与商品大类关联规则 (FP-Growth)**
        *   **目标**: 挖掘不同支付方式与商品**大类**之间的关联规则。
        *   **数据准备**: 类似于Task 1，但购物篮 `basket_items` 通过 `expr("array_distinct(concat(array(payment_method), major_categories_in_transaction))")` 构建，将支付方式也作为购物篮中的一个项。
        *   **FP-Growth**: `minSupport = 0.01`, `minConfidence = 0.6`。
        *   **结果**: 提取频繁项集和关联规则，进行CSV兼容转换后保存。如果找到规则，则进行可视化。
    *   **Task 2.1.b: 支付方式与商品小类关联规则 (FP-Growth)**
        *   **目标**: 挖掘不同支付方式与商品**小类**之间的关联规则。
        *   **数据准备**: 购物篮构建类似于2.1.a，但收集的是 `item_minor_category`。
        *   **FP-Growth**: `minSupport = 0.01`, `minConfidence = 0.6`。
        *   **结果**: 同2.1.a，但针对小类。
    *   **Task 2.2.a: 每个商品小类的支付方式分布 (统计)**
        *   **目标**: 分析对于每个商品**小类**，各种支付方式的使用占比。
        *   **方法**:
            *   按 `(item_minor_category, payment_method)` 分组并计数 (`purchase_count`)。
            *   计算每个 `item_minor_category` 的总购买次数 (`total_category_purchases`)。
            *   计算百分比: `payment_percentage_in_category = (purchase_count / total_category_purchases) * 100`。
            *   结果保存为CSV。
        *   **可视化**: 对Top N个热门小类，绘制分组条形图展示其支付方式分布。
    *   **Task 2.2.b: 每种支付方式购买的商品小类分布 (统计)**
        *   **目标**: 分析对于每种支付方式，其最常用于购买哪些商品**小类**。
        *   **方法**:
            *   复用2.2.a的计数结果。
            *   计算每种 `payment_method` 的总使用次数 (`total_payment_purchases`)。
            *   计算百分比: `category_percentage_in_payment = (purchase_count / total_payment_purchases) * 100`。
            *   结果保存为CSV。
        *   **可视化**: 对每种支付方式，绘制条形图展示其购买的Top N个小类的百分比。
    *   **Task 2.3: 高价值商品支付方式分析 (统计)**
        *   **目标**: 分析单品目录价格 > 5000 的高价值商品的支付方式分布。
        *   **方法**:
            *   筛选 `df_final_preprocessed` 中 `item_unit_price > 5000` 的记录。
            *   按 `payment_method` 分组并计数。
            *   结果保存为CSV。
        *   **可视化**: 绘制垂直条形图展示各支付方式的购买次数。
    *   **Task 2.4: 高价值小类支付方式热力图 (统计)**
        *   **目标**: 对价格 > 5000 的商品，按其**小类**分析支付方式的百分比分布，并用热力图展示。
        *   **方法**:
            *   筛选高价值商品购买记录，保留 `item_minor_category` 和 `payment_method`。
            *   计算每个 `(item_minor_category, payment_method)` 组合的购买次数。
            *   计算每个高价值 `item_minor_category` 的总购买次数。
            *   计算支付方式在每个高价值小类中的使用百分比。
            *   结果保存为CSV。
        *   **可视化**: 将结果数据透视，以小类为行，支付方式为列，百分比为值，绘制热力图。可以筛选Top N个高价值小类进行展示。

*   **4.3 Task 3: 时间序列模式挖掘**
    *   **数据准备 (`temporal_df`)**: 在 `df_final_preprocessed` 基础上，提取 `purchase_year`, `purchase_quarter`, `purchase_month`, `purchase_dayofweek`, `purchase_weekday_name` 等时间特征列。缓存 `temporal_df`。
    *   **Task 3.1: 季节性购物模式 (整体)**
        *   **目标**: 分析整体用户购物行为的季节性。
        *   **方法**: 按年、季度、月、星期几聚合总购买商品数。
        *   **结果**: 保存各时间维度的统计表为CSV。
        *   **可视化**: 为各时间维度绘制条形图展示购买量分布。
    *   **Task 3.2: 特定商品大类购买频率变化**
        *   **目标**: 识别各商品**大类**在不同时间段（月、季度、星期几）的购买频率变化。
        *   **方法**: 在 `temporal_df` 基础上，按 (时间维度, `item_major_category`) 分组聚合购买商品数。
        *   **结果**: 保存各时间维度按大类聚合的统计表为Parquet（因为可能较大）。
        *   **可视化**:
            *   为每个大类（2025年数据除外）绘制月度和季度购买趋势折线图。
            *   绘制一个汇总的分组条形图，比较所有大类在星期几的购买分布。
    *   **Task 3.3: "先A后B"时序模式 (大类，优化版)**
        *   **目标**: 探索用户购买**大类**A后，紧接着的下一个交易中购买大类B的模式。
        *   **数据准备**:
            *   按用户和原始交易标识聚合，得到每个交易的商品大类集合，并按时间排序。
            *   使用 `lag` 窗口函数获取上一个交易的大类集合 (`previous_categories`) 和当前交易的大类集合 (`current_categories`)。
            *   筛选出有效的交易对。
        *   **模式统计 (优化版)**:
            *   使用 `explode` 展开 `previous_categories` 和 `current_categories`。
            *   分子: `groupBy("prev_cat_num", "curr_cat_num").agg(count("*"))` 计算 (上一个交易中的A品类 -> 当前交易中的B品类) 的item-level转换次数。
            *   分母: `groupBy("prev_cat_den").agg(count("*"))` 计算每个品类作为前件（在 `previous_categories` 中）出现的总次数。
            *   计算时序置信度: 分子 / 分母。
            *   结果保存为CSV。
        *   **可视化**: 绘制时序置信度的热力图或Top N规则的条形图。
    *   **清理**: `unpersist()` `temporal_df` 和 `sequential_pairs_df_task3_3`。

*   **4.4 Task 4: 退款模式分析 (大类)**
    *   **目标**: 挖掘与"已退款"或"部分退款"状态相关的商品**大类**组合。
    *   **数据准备**:
        *   筛选 `df_final_preprocessed` 中 `payment_status` 为 "已退款" 或 "部分退款" 的记录。
        *   基于原始交易标识，为每个退款交易构建包含所购商品大类的购物篮。
        *   过滤掉只含少于2个大类的购物篮。
    *   **FP-Growth**: `minSupport = 0.005`, `minConfidence = 0.4`。
    *   **结果**: 提取频繁项集和关联规则，计算支持度、置信度、提升度（均在退款订单的上下文中）。进行CSV兼容转换后保存。
    *   **可视化**: 绘制与退款相关的Top N关联规则的条形图（按置信度和提升度排序）。
    *   **清理**: `unpersist()` 退款购物篮DataFrame。

**5. 最终清理**
*   `unpersist()` `df_final_preprocessed`。
*   `spark.stop()` 停止SparkSession。

**6. 输出文件结构**
所有输出将保存在执行脚本的同级目录下的 `data_mining_results` 文件夹中，内含：
*   `tables/`: 存放所有CSV和Parquet表格结果。
*   `charts/`: 存放所有PNG图表。
